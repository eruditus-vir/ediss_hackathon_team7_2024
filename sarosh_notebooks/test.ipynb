{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Cython>=0.22 and NumPy are required.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pystan\u001b[0m\u001b[31m\n",
      "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[57 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-7qj9on0w/fbprophet_5f1faba46b214a6f8c43076ab91f0ce5/setup.py:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  \u001b[31m   \u001b[0m   from pkg_resources import (\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib\n",
      "  \u001b[31m   \u001b[0m creating build/lib/fbprophet\n",
      "  \u001b[31m   \u001b[0m creating build/lib/fbprophet/stan_model\n",
      "  \u001b[31m   \u001b[0m Note: NumExpr detected 40 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "  \u001b[31m   \u001b[0m NumExpr defaulting to 8 threads.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7qj9on0w/fbprophet_5f1faba46b214a6f8c43076ab91f0ce5/setup.py\", line 122, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/__init__.py\", line 107, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/wheel/bdist_wheel.py\", line 364, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/ediss_data/ediss4/sarosh/anaconda3/envs/apnea/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7qj9on0w/fbprophet_5f1faba46b214a6f8c43076ab91f0ce5/setup.py\", line 48, in run\n",
      "  \u001b[31m   \u001b[0m     build_models(target_dir)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7qj9on0w/fbprophet_5f1faba46b214a6f8c43076ab91f0ce5/setup.py\", line 36, in build_models\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.models import StanBackendEnum\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7qj9on0w/fbprophet_5f1faba46b214a6f8c43076ab91f0ce5/fbprophet/__init__.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.forecaster import Prophet\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7qj9on0w/fbprophet_5f1faba46b214a6f8c43076ab91f0ce5/fbprophet/forecaster.py\", line 17, in <module>\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.make_holidays import get_holiday_names, make_holidays_df\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7qj9on0w/fbprophet_5f1faba46b214a6f8c43076ab91f0ce5/fbprophet/make_holidays.py\", line 14, in <module>\n",
      "  \u001b[31m   \u001b[0m     import fbprophet.hdays as hdays_part2\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7qj9on0w/fbprophet_5f1faba46b214a6f8c43076ab91f0ce5/fbprophet/hdays.py\", line 13, in <module>\n",
      "  \u001b[31m   \u001b[0m     from convertdate.islamic import from_gregorian, to_gregorian\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'convertdate'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for fbprophet\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not build wheels for pystan, fbprophet, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q auto-ts pystan~=2.14 fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#give the absolute path of the repo folder here\n",
    "sys.path.insert(1, '/home/ediss4/sarosh/personal/hackathon/ediss_hackathon_team7_2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.read import load_data_xlsx, read_and_print_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_data_path = '/home/ediss4/sarosh/personal/hackathon/ediss_hackathon_team7_2024/data/Feature_Track.xlsx'\n",
    "simulated_data_path = '/home/ediss4/sarosh/personal/hackathon/ediss_hackathon_team7_2024/data/Feature_Simulation.xlsx'\n",
    "track_data = load_data_xlsx(track_data_path)\n",
    "simulated_data = load_data_xlsx(track_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # parametreleri\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        # batch-layer\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class FTTransformer(keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "            categories,\n",
    "            num_continuous,\n",
    "            dim,\n",
    "            dim_out,\n",
    "            depth,\n",
    "            embedding_dim,\n",
    "            heads,\n",
    "            attn_dropout,\n",
    "            ff_dropout,\n",
    "            mlp_hidden,\n",
    "            normalize_continuous = True):\n",
    "\n",
    "        super(FTTransformer, self).__init__()\n",
    "\n",
    "        # --> continuous inputs\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.normalize_continuous = normalize_continuous\n",
    "        if normalize_continuous:\n",
    "            self.continuous_normalization = layers.LayerNormalization()\n",
    "\n",
    "        # --> categorical inputs\n",
    "\n",
    "        # embedding\n",
    "        self.embedding_layers = []\n",
    "        for number_of_classes in categories:\n",
    "            self.embedding_layers.append(layers.Embedding(input_dim = number_of_classes, output_dim = dim))\n",
    "        # self.embedding_layers_cont = []\n",
    "        # for number_of_classes in range(177):\n",
    "        #     self.embedding_layers_cont.append(layers.Embedding(input_dim = 10, output_dim = dim))\n",
    "        self.embb = layers.Embedding(input_dim = 1000, output_dim = dim)\n",
    "        self.flatten_output = layers.Flatten()\n",
    "\n",
    "        # concatenation\n",
    "        self.embedded_concatenation = layers.Concatenate(axis=1)\n",
    "        self.cont_embedded_concatenation = layers.Concatenate(axis=1)\n",
    "\n",
    "        # adding transformers\n",
    "        self.transformers = []\n",
    "        for _ in range(depth):\n",
    "            self.transformers.append(TransformerBlock(dim, heads, dim))\n",
    "        self.flatten_transformer_output = layers.Flatten()\n",
    "\n",
    "        # --> MLP\n",
    "        self.pre_concatenation = layers.Concatenate(axis=1)\n",
    "\n",
    "        # mlp layers\n",
    "        self.mlp_layers = []\n",
    "        for size, activation in mlp_hidden:\n",
    "            self.mlp_layers.append(layers.Dense(size, activation=activation))\n",
    "\n",
    "        self.output_layer = layers.Dense(dim_out)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        continuous_inputs  = inputs[0]\n",
    "        categorical_inputs = inputs[1:]\n",
    "        # print(continuous_inputs.shape, type(categorical_inputs))\n",
    "        # --> continuous\n",
    "        if self.normalize_continuous:\n",
    "            continuous_inputs = self.continuous_normalization(continuous_inputs)\n",
    "    \n",
    "        # cont_embedding_outputs = []\n",
    "        # print(continuous_inputs.transpoghjse()[0].shape)\n",
    "        # for continuous_input, embedding_layer in zip(tf.unstack(tf.transpose(continuous_inputs)), self.embedding_layers_cont):\n",
    "        #     cont_embedding_outputs.append(tf.expand_dims(embedding_layer(continuous_input),axis=1))\n",
    "        # continuous_inputs = self.cont_embedded_concatenation(cont_embedding_outputs)\n",
    "        continuous_inputs = self.embb(continuous_inputs)\n",
    "        # print(cont_embedding_outputs[0].shape)\n",
    "        # --> categorical\n",
    "        embedding_outputs = []\n",
    "        for categorical_input, embedding_layer in zip(categorical_inputs, self.embedding_layers):\n",
    "            embedding_outputs.append(embedding_layer(categorical_input))\n",
    "        categorical_inputs = self.embedded_concatenation(embedding_outputs)\n",
    "        # print(embedding_outputs[0].shape)\n",
    "        # print(embedding_outputs[0].shape)\n",
    "        # print(categorical_inputs.shape)\n",
    "        trans_input = self.pre_concatenation([continuous_inputs, categorical_inputs])\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            trans_input = transformer(trans_input)\n",
    "        mlp_input = self.flatten_transformer_output(trans_input)\n",
    "        # print(categorical_inputs.shape)\n",
    "        # --> MLP\n",
    "        # print(mlp_input.shape)\n",
    "        for mlp_layer in self.mlp_layers:\n",
    "            mlp_input = mlp_layer(mlp_input)\n",
    "        return self.output_layer(mlp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = FTTransformer(\n",
    "        categories = [], # number of unique elements in each categorical feature\n",
    "        num_continuous = 177,      # number of numerical features\n",
    "        dim = 16,                # embedding/transformer dimension\n",
    "        dim_out = 1,             # dimension of the model output\n",
    "        depth = 6,  \n",
    "        embedding_dim=256,             # number of transformer layers in the stack\n",
    "        heads = 8,               # number of attention heads\n",
    "        attn_dropout = 0.1,      # attention layer dropout in transformers\n",
    "        ff_dropout = 0.1,        # feed-forward layer dropout in transformers\n",
    "        mlp_hidden = [(1024, 'relu'),(256, 'relu'),(128, 'relu'),(64, 'relu'), (32, 'relu')] # mlp layer dimensions and activations\n",
    "    )\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_device():\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # connect to tpu cluster\n",
    "        strategy = tf.distribute.TPUStrategy(tpu) # get strategy for tpu\n",
    "        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n",
    "        device='TPU'\n",
    "    except: # otherwise detect GPUs\n",
    "        tpu = None\n",
    "        gpus = tf.config.list_logical_devices('GPU') # get logical gpus\n",
    "        ngpu = len(gpus)\n",
    "        if ngpu: # if number of GPUs are 0 then CPU\n",
    "            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n",
    "            print(\"> Running on GPU\", end=' | ')\n",
    "            print(\"Num of GPUs: \", ngpu)\n",
    "            device='GPU'\n",
    "        else:\n",
    "            print(\"> Running on CPU\")\n",
    "            strategy = tf.distribute.get_strategy() # connect to single gpu or cpu\n",
    "            device='CPU'\n",
    "    return strategy, device, tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 12:23:48.215905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:18:00.0, compute capability: 7.0\n",
      "2024-02-29 12:23:48.216702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14604 MB memory:  -> device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-02-29 12:23:48.217239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14604 MB memory:  -> device: 2, name: Tesla V100-PCIE-16GB, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2024-02-29 12:23:48.217799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14604 MB memory:  -> device: 3, name: Tesla V100-PCIE-16GB, pci bus id: 0000:af:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on GPU | Num of GPUs:  4\n"
     ]
    }
   ],
   "source": [
    "strategy, device, tpu = configure_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apnea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
